{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run some setup code for this notebook.\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "from cs231n.data_utils import load_CIFAR10\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# This is a bit of magic to make matplotlib figures appear inline in the\n",
    "# notebook rather than in a new window.\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# Some more magic so that the notebook will reload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the raw CIFAR-10 data.\n",
    "cifar10_dir = 'cs231n/datasets/cifar-10-batches-py'\n",
    "X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "num_classes = len(classes)\n",
    "samples_per_class = 5\n",
    "for y, cls in enumerate(classes):\n",
    "    idxs = np.flatnonzero(y_train == y)\n",
    "    idxs = np.random.choice(idxs, samples_per_class, replace = False)\n",
    "    for i, idx in enumerate(idxs):\n",
    "        plot_idx = i * num_classes + y + 1\n",
    "        plt.subplot(samples_per_class, num_classes, plot_idx)\n",
    "        plt.imshow(X_train[idx].astype('uint8'))\n",
    "        plt.axis('off')\n",
    "        if(i == 0):\n",
    "            plt.title(classes[y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_training = 49000\n",
    "num_validation = 1000\n",
    "num_test = 1000\n",
    "num_dev = 500\n",
    "\n",
    "# Our validation set will be num_validation points from the original\n",
    "# training set.\n",
    "mask = range(num_training, num_training + num_validation)\n",
    "X_val = X_train[mask]\n",
    "y_val = y_train[mask]\n",
    "\n",
    "# Our training set will be the first num_train points from the original\n",
    "# training set.\n",
    "mask = range(num_training)\n",
    "X_train = X_train[mask]\n",
    "y_train = y_train[mask]\n",
    "\n",
    "# We will also make a development set, which is a small subset of\n",
    "# the training set.\n",
    "mask = np.random.choice(num_training, num_dev, replace=False)\n",
    "X_dev = X_train[mask]\n",
    "y_dev = y_train[mask]\n",
    "\n",
    "# We use the first num_test points of the original test set as our\n",
    "# test set.\n",
    "mask = range(num_test)\n",
    "X_test = X_test[mask]\n",
    "y_test = y_test[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_image = np.mean(X_train, axis = 0)\n",
    "plt.figure(figsize = (4, 4))\n",
    "plt.imshow(mean_image.reshape(32, 32, 3).astype('uint8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train -= mean_image\n",
    "X_val -= mean_image\n",
    "X_test -= mean_image\n",
    "X_dev -= mean_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
    "X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (X_train.shape, X_val.shape, X_test.shape, X_dev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cs231n.classifiers.linear_svm import svm_loss_naive\n",
    "import time\n",
    "\n",
    "tic = time.time()\n",
    "W = np.random.rand(X_dev.shape[1], 10)*.0001\n",
    "loss, grad = svm_loss_naive(W, X_dev, y_dev, .0001)\n",
    "toc =time.time() - tic\n",
    "print(\"It took {} seconds, with a loss of {:.2f}\".format(toc, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once you've implemented the gradient, recompute it with the code below\n",
    "# and gradient check it with the function we provided for you\n",
    "\n",
    "# Compute the loss and its gradient at W.\n",
    "loss, grad = svm_loss_vectorized(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# Numerically compute the gradient along several randomly chosen dimensions, and\n",
    "# compare them with your analytically computed gradient. The numbers should match\n",
    "# almost exactly along all dimensions.\n",
    "from cs231n.gradient_check import grad_check_sparse\n",
    "f = lambda w: svm_loss_vectorized(w, X_dev, y_dev, 0.0)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad)\n",
    "\n",
    "# do the gradient check once again with regularization turned on\n",
    "# you didn't forget the regularization gradient did you?\n",
    "loss, grad = svm_loss_vectorized(W, X_dev, y_dev, 1e2)\n",
    "f = lambda w: svm_loss_vectorized(w, X_dev, y_dev, 1e2)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inline Question 1:\n",
    "It is possible that once in a while a dimension in the gradcheck will not match exactly. What could such a discrepancy be caused by? Is it a reason for concern? What is a simple example in one dimension where a gradient check could fail? *Hint: the SVM loss function is not strictly speaking differentiable*\n",
    "\n",
    "When we take the scores, the derivative is either 1 or 0, but if we have scores[j] - correct_score + 1 = 0 then, we don't know the derivative. Our implementation would not add an extra -X[i] at the correct column, and the numerical would just be either a 1. So it WOULD have an extra -X[i] in the gradient, while the other one would not have that extra X[i].\n",
    "\n",
    "One example could be \n",
    "x = [1, -2]\n",
    "W = [[-1, 2],\n",
    "     [0, 1]]\n",
    "\n",
    "In this case, we have x dot W[0] be -1, and x dot W[1] be 0, meaning -1 - 0 + 1 = 0. \n",
    "This would cause the max(0, 0) to be non-differentiable at that point. \n",
    "\n",
    "Gradcheck would calculate that we have ( f(W + h) - f(W) ) / h. For each value in the first column, increasing it by a nudge h would cause it to either be > 0 (with (-1 + h)\\*1) or < 0 (with (0 + h)\\*-2). \n",
    "\n",
    "When we take it together we get (-1 + h)*1 + (h)*-2 = -1 + h - 2h = -1 - h. Thus, we will have a value that is < 0. So when we take the loss function at f(W) we get 0, with f(W + h) we get -h so we have -h/h = -1.\n",
    "\n",
    "In the case of our \"analytical\" gradient, we would just have a score of -1 - 0 + 1 = 0 so we don't actually do anything to our zero gradient. \n",
    "\n",
    "Overall, we would just have our numerical be -1 and our analytical be 0. But alas, there is no need to despair!\n",
    "This would only happen if our score was 0 <= score < h, meaning this is very unlikely to happen! So code on dear child."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive loss: 8.991033e+00 computed in 0.208981s\n",
      "Vectorized loss: 8.991033e+00 computed in 0.020474s\n",
      "difference: -0.000000\n"
     ]
    }
   ],
   "source": [
    "tic = time.time()\n",
    "loss_naive, grad_naive = svm_loss_naive(W, X_dev, y_dev, 0.00001)\n",
    "toc = time.time()\n",
    "print ('Naive loss: %e computed in %fs' % (loss_naive, toc - tic))\n",
    "\n",
    "from cs231n.classifiers.linear_svm import svm_loss_vectorized\n",
    "tic = time.time()\n",
    "loss_vectorized, _ = svm_loss_vectorized(W, X_dev, y_dev, 0.00001)\n",
    "toc = time.time()\n",
    "print ('Vectorized loss: %e computed in %fs' % (loss_vectorized, toc - tic))\n",
    "\n",
    "# The losses should match but your vectorized implementation should be much faster.\n",
    "print ('difference: %f' % (loss_naive - loss_vectorized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive loss and gradient: computed in 16.768927s\n",
      "Vectorized loss and gradient: computed in 0.418776s\n",
      "difference: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Complete the implementation of svm_loss_vectorized, and compute the gradient\n",
    "# of the loss function in a vectorized way.\n",
    "\n",
    "# The naive implementation and the vectorized implementation should match, but\n",
    "# the vectorized version should still be much faster.\n",
    "tic = time.time()\n",
    "_, grad_naive = svm_loss_naive(W, X_dev, y_dev, 0.00001)\n",
    "toc = time.time()\n",
    "print ('Naive loss and gradient: computed in %fs' % (toc - tic))\n",
    "\n",
    "tic = time.time()\n",
    "_, grad_vectorized = svm_loss_vectorized(W, X_dev, y_dev, 0.00001)\n",
    "toc = time.time()\n",
    "print ('Vectorized loss and gradient: computed in %fs' % (toc - tic))\n",
    "\n",
    "# The loss is a single number, so it is easy to compare the values computed\n",
    "# by the two implementations. The gradient on the other hand is a matrix, so\n",
    "# we use the Frobenius norm to compare them.\n",
    "difference = np.linalg.norm(grad_naive - grad_vectorized, ord='fro')\n",
    "print ('difference: %f' % difference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the other method:\n",
    "Naive loss and gradient: computed in 14.499156s\n",
    "Vectorized loss and gradient: computed in 0.339304s\n",
    "\n",
    "With my method:\n",
    "Naive loss and gradient: computed in 14.800224s\n",
    "Vectorized loss and gradient: computed in 0.283880s\n",
    "    \n",
    "So my method is slightly faster\n",
    "But why?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the validation set to tune hyperparameters (regularization strength and\n",
    "# learning rate). You should experiment with different ranges for the learning\n",
    "# rates and regularization strengths; if you are careful you should be able to\n",
    "# get a classification accuracy of about 0.4 on the validation set.\n",
    "learning_rates = [1e-7]\n",
    "regularization_strengths = np.arange(1e5,1e6,1e4*6)\n",
    "print(\"Reg strengths are \", regularization_strengths)\n",
    "# results is dictionary mapping tuples of the form\n",
    "# (learning_rate, regularization_strength) to tuples of the form\n",
    "# (training_accuracy, validation_accuracy). The accuracy is simply the fraction\n",
    "# of data points that are correctly classified.\n",
    "results = {}\n",
    "best_val = -1   # The highest validation accuracy that we have seen so far.\n",
    "best_svm = None # The LinearSVM object that achieved the highest validation rate.\n",
    "\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Write code that chooses the best hyperparameters by tuning on the validation #\n",
    "# set. For each combination of hyperparameters, train a linear SVM on the      #\n",
    "# training set, compute its accuracy on the training and validation sets, and  #\n",
    "# store these numbers in the results dictionary. In addition, store the best   #\n",
    "# validation accuracy in best_val and the LinearSVM object that achieves this  #\n",
    "# accuracy in best_svm.                                                        #\n",
    "#                                                                              #\n",
    "# Hint: You should use a small value for num_iters as you develop your         #\n",
    "# validation code so that the SVMs don't take much time to train; once you are #\n",
    "# confident that your validation code works, you should rerun the validation   #\n",
    "# code with a larger value for num_iters.                                      #\n",
    "################################################################################\n",
    "for i, lr in enumerate(learning_rates):\n",
    "    for j, rs in enumerate(regularization_strengths):\n",
    "        svm = LinearSVM()\n",
    "        loss_hist = svm.train(X_train, y_train, learning_rate=lr, reg=rs,\n",
    "                      num_iters=150, verbose=True)\n",
    "        test_pred = svm.predict(X_test)\n",
    "        #plt.plot(loss_hist)\n",
    "        #plt.title(\"Loss history for lr: {} and rs: {}\".format(lr, rs))\n",
    "        #plt.show()\n",
    "        test_loss = np.mean(test_pred == y_test)\n",
    "        val_pred = svm.predict(X_val)\n",
    "        val_loss = np.mean(val_pred == y_val)\n",
    "        if val_loss < best_val:\n",
    "            best_val = val_loss\n",
    "            best_lr = lr\n",
    "            best_rs = rs\n",
    "        results[(lr, rs)] = (test_loss, val_loss)\n",
    "################################################################################\n",
    "#                              END OF YOUR CODE                                #\n",
    "################################################################################\n",
    "    \n",
    "# Print out results.\n",
    "for lr, reg in sorted(results):\n",
    "    train_accuracy, val_accuracy = results[(lr, reg)]\n",
    "    best_val = (val_accuracy) if best_val < val_accuracy else best_val\n",
    "    print ('lr %e reg %e train accuracy: %f val accuracy: %f' % (\n",
    "                lr, reg, train_accuracy, val_accuracy))\n",
    "    \n",
    "print ('best validation accuracy achieved during cross-validation: %f' % best_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the file linear_classifier.py, implement SGD in the function\n",
    "# LinearClassifier.train() and then run it with the code below.\n",
    "from cs231n.classifiers import LinearSVM\n",
    "svm = LinearSVM()\n",
    "tic = time.time()\n",
    "loss_hist = svm.train(X_train, y_train, learning_rate=1e-7, reg=5e4,\n",
    "                      num_iters=1000, verbose=True)\n",
    "toc = time.time()\n",
    "print ('That took %fs' % (toc - tic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_hist)\n",
    "plt.xlabel(\"iteration number\")\n",
    "plt.ylabel(\"error\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = svm.predict(X_train)\n",
    "print(\"The percentage correct on training set: {:.2f}%\".format(np.mean(y_pred_train == y_train)))\n",
    "print(np.sum(y_pred_train == y_train))\n",
    "print(len(y_train))\n",
    "y_pred_val = svm.predict(X_val)\n",
    "print(\"The percentage correct on validation set: {:.2f}%\".format(np.mean(y_pred_val == y_val)))\n",
    "print(np.sum(y_pred_val == y_val))\n",
    "print(len(y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the LinearSVM.predict function and evaluate the performance on both the\n",
    "# training and validation set\n",
    "y_train_pred = svm.predict(X_train)\n",
    "print ('training accuracy: %f' % (np.mean(y_train == y_train_pred), ))\n",
    "y_val_pred = svm.predict(X_val)\n",
    "print ('validation accuracy: %f' % (np.mean(y_val == y_val_pred), ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_lr = 1e-7\n",
    "best_rs = 2.6e5\n",
    "\n",
    "svm = LinearSVM()\n",
    "loss_hist = svm.train(X_train, y_train, learning_rate=best_lr, reg=best_rs,\n",
    "                      num_iters=1500, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = svm.predict(X_train)\n",
    "test_loss = np.mean(test_pred == y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the actual results\n",
    "import math\n",
    "x_scatter = [math.log10(x[0]) for x in results]\n",
    "y_scatter = [math.log10(x[1]) for x in results]\n",
    "\n",
    "colors = [results[x][0] for x in results]\n",
    "plt.subplot(2, 1, 1)\n",
    "marker_width = 100\n",
    "plt.scatter(x_scatter, y_scatter, marker_width, c = colors)\n",
    "plt.colorbar()\n",
    "plt.title(\"Test data results\")\n",
    "plt.xlabel(\"lr\")\n",
    "plt.ylabel(\"regularization loss\")\n",
    "plt.show()\n",
    "\n",
    "colors = [results[x][1] for x in results]\n",
    "plt.subplot(2, 1, 2)\n",
    "marker_width = 100\n",
    "plt.scatter(x_scatter, y_scatter, marker_width, c = colors)\n",
    "plt.colorbar()\n",
    "plt.title(\"Validation data loss\")\n",
    "plt.xlabel(\"lr\")\n",
    "plt.ylabel(\"regularization loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "x_scatter = [math.log10(x[0]) for x in results]\n",
    "y_scatter = [math.log10(x[1]) for x in results]\n",
    "\n",
    "# plot training accuracy\n",
    "marker_size = 100\n",
    "colors = [results[x][0] for x in results]\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.scatter(x_scatter, y_scatter, marker_size, c=colors)\n",
    "plt.colorbar()\n",
    "plt.xlabel('log learning rate')\n",
    "plt.ylabel('log regularization strength')\n",
    "plt.title('CIFAR-10 training accuracy')\n",
    "\n",
    "# plot validation accuracy\n",
    "colors = [results[x][1] for x in results] # default size of markers is 20\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.scatter(x_scatter, y_scatter, marker_size, c=colors)\n",
    "plt.colorbar()\n",
    "plt.xlabel('log learning rate')\n",
    "plt.ylabel('log regularization strength')\n",
    "plt.title('CIFAR-10 validation accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49000, 3073)"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize the weights by taking the \"average\" picture for each class\n",
    "w = "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
